{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e5f195",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00e5f195",
    "outputId": "3723a583-1ead-4047-f4a0-7671dfa03f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.5.0\n",
      "  Downloading openai-1.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from openai==1.5.0) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from openai==1.5.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from openai==1.5.0) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from openai==1.5.0) (1.10.12)\n",
      "Requirement already satisfied: sniffio in c:\\users\\karth\\anaconda3\\lib\\site-packages (from openai==1.5.0) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from openai==1.5.0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from openai==1.5.0) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai==1.5.0) (3.4)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\karth\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai==1.5.0) (1.0.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\karth\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.5.0) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\karth\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.5.0) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.5.0) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\karth\\anaconda3\\lib\\site-packages (from tqdm>4->openai==1.5.0) (0.4.6)\n",
      "Downloading openai-1.5.0-py3-none-any.whl (223 kB)\n",
      "   ---------------------------------------- 0.0/223.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 223.7/223.7 kB 6.9 MB/s eta 0:00:00\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.1\n",
      "    Uninstalling openai-0.28.1:\n",
      "      Successfully uninstalled openai-0.28.1\n",
      "Successfully installed openai-1.5.0\n",
      "Collecting langchain==0.0.351\n",
      "  Downloading langchain-0.0.351-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from langchain==0.0.351) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from langchain==0.0.351) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from langchain==0.0.351) (3.8.6)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from langchain==0.0.351) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.351)\n",
      "  Downloading dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.351)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.2 (from langchain==0.0.351)\n",
      "  Downloading langchain_community-0.0.15-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting langchain-core<0.2,>=0.1 (from langchain==0.0.351)\n",
      "  Downloading langchain_core-0.1.15-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.70 (from langchain==0.0.351)\n",
      "  Downloading langsmith-0.0.83-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from langchain==0.0.351) (1.26.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from langchain==0.0.351) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from langchain==0.0.351) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.351)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.351) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.351) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.351) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.351) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.351) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.351) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.351)\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.351)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.351) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain==0.0.351) (3.7.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain==0.0.351) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain==0.0.351) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.0.351) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.0.351) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.0.351) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.351) (2.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain==0.0.351) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\karth\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain==0.0.351) (1.0.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.351) (1.0.0)\n",
      "Downloading langchain-0.0.351-py3-none-any.whl (794 kB)\n",
      "   ---------------------------------------- 0.0/794.3 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 204.8/794.3 kB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 794.3/794.3 kB 10.1 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.15-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 50.7 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.1.15-py3-none-any.whl (229 kB)\n",
      "   ---------------------------------------- 0.0/229.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 229.6/229.6 kB ? eta 0:00:00\n",
      "Downloading langsmith-0.0.83-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.3/49.3 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.4/49.4 kB ? eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, tenacity, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-community, langchain\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.0.1\n",
      "    Uninstalling tenacity-8.0.1:\n",
      "      Successfully uninstalled tenacity-8.0.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 langchain-0.0.351 langchain-community-0.0.15 langchain-core-0.1.15 langsmith-0.0.83 marshmallow-3.20.2 tenacity-8.2.3 typing-inspect-0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface-hub==0.19.4\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\karth\\anaconda3\\lib\\site-packages (from huggingface-hub==0.19.4) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from huggingface-hub==0.19.4) (2023.9.2)\n",
      "Requirement already satisfied: requests in c:\\users\\karth\\anaconda3\\lib\\site-packages (from huggingface-hub==0.19.4) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from huggingface-hub==0.19.4) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from huggingface-hub==0.19.4) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from huggingface-hub==0.19.4) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from huggingface-hub==0.19.4) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\karth\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub==0.19.4) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.19.4) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.19.4) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.19.4) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.19.4) (2023.7.22)\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 266.2/311.7 kB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 311.7/311.7 kB 6.4 MB/s eta 0:00:00\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "Successfully installed huggingface-hub-0.19.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.19.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken==0.5.2\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from tiktoken==0.5.2) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from tiktoken==0.5.2) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karth\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.2) (2023.7.22)\n",
      "Downloading tiktoken-0.5.2-cp310-cp310-win_amd64.whl (786 kB)\n",
      "   ---------------------------------------- 0.0/786.3 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 276.5/786.3 kB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 786.3/786.3 kB 9.9 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.2\n"
     ]
    }
   ],
   "source": [
    "#Please install the package as per your requirement :)\n",
    "!pip install openai==1.5.0\n",
    "!pip install langchain==0.0.351\n",
    "!pip install huggingface-hub==0.19.4\n",
    "!pip install tiktoken==0.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be0f78",
   "metadata": {
    "id": "b2be0f78"
   },
   "source": [
    "![FGLBtqjXMAEqx_v.jpg](attachment:FGLBtqjXMAEqx_v.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c5dc6",
   "metadata": {
    "id": "9b8c5dc6"
   },
   "source": [
    "<font color=\"blue\">\n",
    "    <b>Conversational memory</b> is what enables chatbots to respond to our queries in a conversational manner. It allows for coherent conversations by considering past interactions, rather than treating each query as independent. Without conversational memory, chatbots would lack the ability to remember and build upon previous interactions.\n",
    "\n",
    "By default, chatbot agents are stateless, meaning they process each incoming query as a separate input, without any knowledge of past interactions. They only focus on the current input and don't retain any information from previous interactions.\n",
    "\n",
    "However, in applications like chatbots, it is crucial to remember past interactions. Conversational memory facilitates this by allowing the agent to recall and utilize information from previous conversations.\n",
    "\n",
    "In summary, conversational memory is essential for chatbots to maintain context, recall past interactions, and engage in more meaningful and coherent conversations with users.\n",
    "    <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a078dc5",
   "metadata": {
    "id": "8a078dc5"
   },
   "source": [
    "![Prompt.png](attachment:Prompt.png)\n",
    "Image Source: Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3db47",
   "metadata": {
    "id": "8aa3db47"
   },
   "source": [
    "# Different Types of Memory :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b915a",
   "metadata": {
    "id": "972b915a"
   },
   "source": [
    "![memories-2.png](attachment:memories-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a81736",
   "metadata": {
    "id": "b6a81736"
   },
   "outputs": [],
   "source": [
    "#from langchain import OpenAI  #Langchain has recently suggested to use the below import\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
    "                                                  ConversationSummaryMemory,\n",
    "                                                  ConversationBufferWindowMemory\n",
    "                                                  )\n",
    "\n",
    "import tiktoken\n",
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62808c2a",
   "metadata": {
    "id": "62808c2a"
   },
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Tiktoken, developed by OpenAI, is a tool used for text tokenization.\n",
    "\n",
    "Tokenization involves dividing a text into smaller units, such as letters or words.\n",
    "Tiktoken allows you to count tokens and estimate the cost of using the OpenAI API, which is based on token usage. It utilizes byte pair encoding (BPE), a compression algorithm that replaces frequently occurring pairs of bytes with a single byte.\n",
    "\n",
    "In summary, Tiktoken helps with efficient text processing, token counting, and cost estimation for using OpenAI's API.\n",
    "    \n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83c6bb7",
   "metadata": {
    "id": "a83c6bb7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "893f42ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "893f42ab",
    "outputId": "af96fa14-053f-4a75-d372-0f966e05090c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo'  #'text-davinci-003' - This model has been depreciated\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f521b4b",
   "metadata": {
    "id": "8f521b4b"
   },
   "source": [
    "<font color=\"blue\">\n",
    "    <b>What is a Memory?</b>\n",
    "    <br>\n",
    "    Chains and Agents operate as stateless, treating each query independently. However, in applications like chatbots, its crucial to remember past interactions. The concept of \"Memory\" serves that purpose.\n",
    "    <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de756f4",
   "metadata": {
    "id": "7de756f4"
   },
   "source": [
    "## Different Types Of Memories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52417cc",
   "metadata": {
    "id": "d52417cc"
   },
   "source": [
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3576f",
   "metadata": {
    "id": "eba3576f"
   },
   "source": [
    "<font color=\"blue\">\n",
    "Imagine you're having a conversation with someone, and you want to remember what you've discussed so far.\n",
    "   <br>\n",
    "The ConversationBufferMemory does exactly that in a chatbot or similar system. It keeps a record, or \"buffer,\" of the past parts of the conversation.\n",
    "    \n",
    "<br>This buffer is an essential part of the context, which helps the chatbot generate better responses. The unique thing about this memory is that it stores the previous conversation exactly as they were, without any changes. <br><br>\n",
    "    <b>It preserves the raw form of the conversation, allowing the chatbot to refer back to specific parts accurately. In summary, the ConversationBufferMemory helps the chatbot remember the conversation history, enhancing the overall conversational experience.\n",
    "     <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91486db7",
   "metadata": {
    "id": "91486db7"
   },
   "source": [
    "<font color=\"blue\">\n",
    "<b>Pros of ConversationBufferMemory:</b>\n",
    "    \n",
    "    *Complete conversation history: It retains the entire conversation history, ensuring comprehensive context for the chatbot.\n",
    "    Accurate references: By storing conversation excerpts in their original form, it enables precise referencing to past interactions, enhancing accuracy.\n",
    "    *Contextual understanding: The preserved raw form of the conversation helps the chatbot maintain a deep understanding of the ongoing dialogue.\n",
    "    *Enhanced responses: With access to the complete conversation history, the chatbot can generate more relevant and coherent responses.\n",
    "     \n",
    "    \n",
    "**Cons of ConversationBufferMemory:**\n",
    "\n",
    "    *Increased memory usage: Storing the entire conversation history consumes memory resources, potentially leading to memory constraints.\n",
    "    *Potential performance impact: Large conversation buffers may slow down processing and response times, affecting the overall system performance.\n",
    "    *Limited scalability: As the conversation grows, the memory requirements and processing load may become impractical for extremely long conversations.\n",
    "    *Privacy concerns: Storing the entire conversation history raises privacy considerations, as sensitive or personal information may be retained in the buffer.\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec567bb",
   "metadata": {
    "id": "fec567bb"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648e411",
   "metadata": {
    "id": "c648e411"
   },
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Let's have a look at the prompt template that is being sent to the LLM     <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5e320a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9f5e320a",
    "outputId": "02ab3ca9-63f7-4844-82d1-9a12d9256261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b5bd55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88b5bd55",
    "outputId": "3571b717-b6a7-420e-b9cf-c4563bd577fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Good morning AI!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': 'Good morning! How can I assist you today?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "883f79d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "883f79d2",
    "outputId": "7c21b4a4-9a59-424b-fc23-29bdc099cb24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Good morning AI!\n",
      "AI: Good morning! How can I assist you today?\n",
      "Human: My name is sharath!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My name is sharath!',\n",
       " 'history': 'Human: Good morning AI!\\nAI: Good morning! How can I assist you today?',\n",
       " 'response': 'Nice to meet you, Sharath! How can I assist you today?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"My name is sharath!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c55d386",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "5c55d386",
    "outputId": "73bbedaf-0f1e-4896-97a2-37dc9d8a8f85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Good morning AI!\n",
      "AI: Good morning! How can I assist you today?\n",
      "Human: My name is sharath!\n",
      "AI: Nice to meet you, Sharath! How can I assist you today?\n",
      "Human: I stay in hyderabad, India\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'That\\'s great! Hyderabad is a beautiful city in India known for its rich history and culture. It is the capital of the Indian state of Telangana and is also known as the \"City of Pearls\" due to its historic role in the pearl trade. Hyderabad is famous for its iconic landmarks such as the Charminar and the Golconda Fort. It is also known for its delicious cuisine, especially the famous Hyderabadi biryani. Is there anything specific you would like to know about Hyderabad?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I stay in hyderabad, India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2122c",
   "metadata": {
    "id": "aea2122c"
   },
   "source": [
    "<font color=\"blue\">\n",
    "Our LLM with ConversationBufferMemory has a special ability to remember past conversations.\n",
    "    \n",
    "<b>It stores and saves the earlier interactions, allowing it to recall and use that information to make better responses. It ensures that the chatbot can keep track of the conversation history for a more coherent experience.   <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc7fb820",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc7fb820",
    "outputId": "aa98b424-be0c-4b2e-9fc9-bca6d21312be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI!\n",
      "AI: Good morning! How can I assist you today?\n",
      "Human: My name is sharath!\n",
      "AI: Nice to meet you, Sharath! How can I assist you today?\n",
      "Human: I stay in hyderabad, India\n",
      "AI: That's great! Hyderabad is a beautiful city in India known for its rich history and culture. It is the capital of the Indian state of Telangana and is also known as the \"City of Pearls\" due to its historic role in the pearl trade. Hyderabad is famous for its iconic landmarks such as the Charminar and the Golconda Fort. It is also known for its delicious cuisine, especially the famous Hyderabadi biryani. Is there anything specific you would like to know about Hyderabad?\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "352be49d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "352be49d",
    "outputId": "4d8fe8b3-0d9b-47c4-86a7-5e162b82b3f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Good morning AI!\n",
      "AI: Good morning! How can I assist you today?\n",
      "Human: My name is sharath!\n",
      "AI: Nice to meet you, Sharath! How can I assist you today?\n",
      "Human: I stay in hyderabad, India\n",
      "AI: That's great! Hyderabad is a beautiful city in India known for its rich history and culture. It is the capital of the Indian state of Telangana and is also known as the \"City of Pearls\" due to its historic role in the pearl trade. Hyderabad is famous for its iconic landmarks such as the Charminar and the Golconda Fort. It is also known for its delicious cuisine, especially the famous Hyderabadi biryani. Is there anything specific you would like to know about Hyderabad?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Your name is Sharath.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f4c896",
   "metadata": {
    "id": "c1f4c896"
   },
   "source": [
    "### ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2787ee4",
   "metadata": {
    "id": "d2787ee4"
   },
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Imagine you have a limited space in your memory to remember recent conversations.\n",
    "<b>   \n",
    "<br>The ConversationBufferWindowMemory is like having a short-term memory that only keeps track of the most recent interactions. It intentionally drops the oldest ones to make room for new ones.\n",
    "</b>\n",
    "<br><br>This helps manage the memory load and reduces the number of tokens used. The important thing is that it still keeps the latest parts of the conversation in their original form, without any modifications.\n",
    "<br>So, it retains the most recent information for the chatbot to refer to, ensuring a more efficient and up-to-date conversation experience.\n",
    "<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebda44",
   "metadata": {
    "id": "36ebda44"
   },
   "source": [
    "<font color=\"blue\">\n",
    "<b>Pros of ConversationBufferWindowMemory:</b>\n",
    "\n",
    "    *Efficient memory utilization: It maintains a limited memory space by only retaining the most recent interactions, optimizing memory usage.\n",
    "    *Reduced token count: Dropping the oldest interactions helps to keep the token count low, preventing potential token limitations.\n",
    "    *Unmodified context retention: The latest parts of the conversation are preserved in their original form, ensuring accurate references and contextual understanding.\n",
    "    *Up-to-date conversations: By focusing on recent interactions, it allows the chatbot to stay current and provide more relevant responses.\n",
    "    \n",
    "**Cons of ConversationBufferWindowMemory:**\n",
    "\n",
    "    *Limited historical context: Since older interactions are intentionally dropped, the chatbot loses access to the complete conversation history, potentially impacting long-term context and accuracy.\n",
    "    *Loss of older information: Valuable insights or details from earlier interactions are not retained, limiting the chatbot's ability to refer back to past conversations.\n",
    "    *Reduced depth of understanding: Without the full conversation history, the chatbot may have a shallower understanding of the user's context and needs.\n",
    "    *Potential loss of context relevance: Important information or context from older interactions may be disregarded, affecting the chatbot's ability to provide comprehensive responses in certain scenarios.\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3a3db18",
   "metadata": {
    "id": "a3a3db18"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41220d",
   "metadata": {
    "id": "fe41220d"
   },
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Let's have a look at the prompt template that is being sent to the LLM     <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a160bfa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a160bfa",
    "outputId": "e057cceb-6885-402d-8c73-0ddd32d92ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70f2d1a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "70f2d1a4",
    "outputId": "0c06f82f-52fa-435a-b175-5bef2d3533e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Good morning AI!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Good morning! How can I assist you today?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d65f588",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "5d65f588",
    "outputId": "35623907-3f3f-458a-ee3a-f175cf9d39c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Good morning AI!\n",
      "AI: Good morning! How can I assist you today?\n",
      "Human: My Name is sharath\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Nice to meet you, Sharath! How can I assist you today?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"My Name is sharath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c43a54b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "c43a54b9",
    "outputId": "105328c5-3069-44f4-ee74-f6eb4a015dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Good morning AI!\n",
      "AI: Good morning! How can I assist you today?\n",
      "Human: My Name is sharath\n",
      "AI: Nice to meet you, Sharath! How can I assist you today?\n",
      "Human: I stay in hyderabad, India\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'That\\'s great! Hyderabad is a beautiful city in India known for its rich history and culture. It is the capital of the Indian state of Telangana and is also known as the \"City of Pearls\" due to its historic role in the pearl trade. Hyderabad is famous for its iconic landmarks such as the Charminar and the Golconda Fort. It is also known for its delicious cuisine, especially the famous Hyderabadi biryani. Is there anything specific you would like to know about Hyderabad?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I stay in hyderabad, India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11717b83",
   "metadata": {
    "id": "11717b83"
   },
   "source": [
    "<font color=\"blue\">\n",
    "Our LLM with ConversationBufferWindowMemory has a special ability to remember past conversations.\n",
    "    <br><br>\n",
    "<b>It stores and saves the earlier interactions based on our choice of K which simply means the number of past conversions basically a count, allowing it to recall and use that information to make better responses. It ensures that the chatbot can keep track of the conversation history for a more coherent experience.   <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03d3bc40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03d3bc40",
    "outputId": "4384a6b2-3df6-4e7f-c68d-3ba04ec1036e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI!\n",
      "AI: Good morning! How can I assist you today?\n",
      "Human: My Name is sharath\n",
      "AI: Nice to meet you, Sharath! How can I assist you today?\n",
      "Human: I stay in hyderabad, India\n",
      "AI: That's great! Hyderabad is a beautiful city in India known for its rich history and culture. It is the capital of the Indian state of Telangana and is also known as the \"City of Pearls\" due to its historic role in the pearl trade. Hyderabad is famous for its iconic landmarks such as the Charminar and the Golconda Fort. It is also known for its delicious cuisine, especially the famous Hyderabadi biryani. Is there anything specific you would like to know about Hyderabad?\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6abb6f11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "6abb6f11",
    "outputId": "b8c5e882-e34e-4b8a-c216-dc3961637c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Good morning AI!\n",
      "AI: Good morning! How can I assist you today?\n",
      "Human: My Name is sharath\n",
      "AI: Nice to meet you, Sharath! How can I assist you today?\n",
      "Human: I stay in hyderabad, India\n",
      "AI: That's great! Hyderabad is a beautiful city in India known for its rich history and culture. It is the capital of the Indian state of Telangana and is also known as the \"City of Pearls\" due to its historic role in the pearl trade. Hyderabad is famous for its iconic landmarks such as the Charminar and the Golconda Fort. It is also known for its delicious cuisine, especially the famous Hyderabadi biryani. Is there anything specific you would like to know about Hyderabad?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Your name is Sharath.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d0357",
   "metadata": {
    "id": "4b6d0357"
   },
   "source": [
    "### ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f0deb",
   "metadata": {
    "id": "f99f0deb"
   },
   "source": [
    "<font color=\"blue\">\n",
    "With the ConversationBufferMemory, the length of the conversation keeps increasing, which can become a problem if it becomes too large for our LLM to handle.\n",
    "    \n",
    "<br>To overcome this, we introduce ConversationSummaryMemory. It keeps a summary of our past conversation snippets as our history. But how does it summarize? Here comes the LLM to the rescue! The LLM (Language Model) helps in condensing or summarizing the conversation, capturing the key information.\n",
    "    \n",
    "<br>So, instead of storing the entire conversation, we store a summarized version. This helps manage the token count and allows the LLM to process the conversation effectively. In summary, ConversationSummaryMemory keeps a condensed version of previous conversations using the power of LLM summarization.\n",
    "\n",
    "<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbffa020",
   "metadata": {
    "id": "dbffa020"
   },
   "source": [
    "<font color=\"blue\">\n",
    "    <b>Pros of ConversationSummaryMemory:</b>\n",
    "\n",
    "    *Efficient memory management- It keeps the conversation history in a summarized form, reducing the memory load.\n",
    "    *Improved processing- By condensing the conversation snippets, it makes it easier for the language model to process and generate responses.\n",
    "    *Avoiding maxing out limitations- It helps prevent exceeding the token count limit, ensuring the prompt remains within the processing capacity of the model.\n",
    "    *Retains important information- The summary captures the essential aspects of previous interactions, allowing for relevant context to be maintained.\n",
    "    \n",
    "\n",
    "<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604901a3",
   "metadata": {
    "id": "604901a3"
   },
   "source": [
    "<font color=\"blue\">\n",
    "    <b>Cons of ConversationSummaryMemory:</b>\n",
    "\n",
    "\n",
    "    *Potential loss of detail: Since the conversation is summarized, some specific details or nuances from earlier interactions might be omitted.\n",
    "    *Reliance on summarization quality: The accuracy and effectiveness of the summarization process depend on the language model's capability, which might introduce potential errors or misinterpretations.\n",
    "    *Limited historical context: Due to summarization, the model's access to the complete conversation history may be limited, potentially impacting the depth of understanding.\n",
    "    *Reduced granularity: The summarized form may lack the fine-grained information present in the original conversation, potentially affecting the accuracy of responses in certain scenarios.\n",
    "    \n",
    "\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387302b",
   "metadata": {
    "id": "3387302b"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationSummaryMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28892797",
   "metadata": {
    "id": "28892797"
   },
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Let's have a look at the prompt template that is being sent to the LLM     <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c398497",
   "metadata": {
    "id": "9c398497",
    "outputId": "12fab3ac-e32c-4ccb-c372-a51e56edc41a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac283e3",
   "metadata": {
    "id": "eac283e3"
   },
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Let's have a look at the prompt template that is being sent out to LLM for our summarization task\n",
    "    \n",
    " **Example : memory=ConversationSummaryMemory(llm=llm)<font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8ba78",
   "metadata": {
    "id": "2df8ba78",
    "outputId": "5ac24f46-95cb-4851-92b6-b951c685d1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0a898",
   "metadata": {
    "id": "cde0a898",
    "outputId": "fe821ec2-d1e8-4d3a-d739-c2e8a33bbc25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Good morning AI!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Good morning! How can I assist you today?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877e7d1",
   "metadata": {
    "id": "2877e7d1",
    "outputId": "f3cc3e2a-b35d-4397-a972-6878bc65aa93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI and the AI responds politely, asking how it can assist the human.\n",
      "Human: My Name is sharath\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Sharath! How can I assist you today?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"My Name is sharath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef49ae7",
   "metadata": {
    "id": "bef49ae7",
    "outputId": "d4f6d5df-f465-4934-db78-611b38c3cf97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI and the AI responds politely, asking how it can assist the human. The AI addresses the human by name, Sharath, and asks how it can assist him today.\n",
      "Human: I stay in hyderabad, India\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Hyderabad is a vibrant city known for its rich history and culture. How can I assist you today, Sharath?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I stay in hyderabad, India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1581d80",
   "metadata": {
    "id": "e1581d80",
    "outputId": "cd2f1c6a-8a40-423e-dd6b-887686155083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human greets the AI and the AI responds politely, asking how it can assist the human. The AI addresses the human by name, Sharath, and mentions that Hyderabad is a vibrant city known for its rich history and culture. The AI asks how it can assist Sharath today.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29df41b",
   "metadata": {
    "id": "b29df41b",
    "outputId": "87ec8359-e116-462e-ed15-03537ff9c8a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI and the AI responds politely, asking how it can assist the human. The AI addresses the human by name, Sharath, and mentions that Hyderabad is a vibrant city known for its rich history and culture. The AI asks how it can assist Sharath today.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Sharath. How can I assist you today, Sharath?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac2d48",
   "metadata": {
    "id": "edac2d48"
   },
   "source": [
    "### ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148eaab8",
   "metadata": {
    "id": "148eaab8"
   },
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "ConversationTokenBufferMemory is a memory mechanism that stores recent interactions in a buffer within the system's memory.\n",
    "<br>\n",
    "Unlike other methods that rely on the number of interactions, this memory system determines when to clear or flush interactions based on the length of tokens used.\n",
    "<br><br>Tokens are units of text, like words or characters, and the buffer is cleared when the token count exceeds a certain threshold. By using token length as a criterion, the memory system ensures that the buffer remains manageable in terms of memory usage.\n",
    "    \n",
    "<br>This approach helps maintain efficient memory management and enables the system to handle conversations of varying lengths effectively.\n",
    "<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4123ad",
   "metadata": {
    "id": "ae4123ad"
   },
   "source": [
    "<font color=\"blue\">\n",
    "    <b>Pros of ConversationTokenBufferMemory:</b>\n",
    "\n",
    "    *Efficient memory management: By using token length instead of the number of interactions, the memory system optimizes memory usage and prevents excessive memory consumption.\n",
    "    *Flexible buffer size: The system adapts to conversations of varying lengths, ensuring that the buffer remains manageable and scalable.\n",
    "    *Accurate threshold determination: Flushing interactions based on token count provides a more precise measure of memory usage, resulting in a better balance between memory efficiency and retaining relevant context.\n",
    "    *Improved system performance: With efficient memory utilization, the overall performance of the system, including response times and processing speed, can be enhanced.\n",
    "    \n",
    "<b>Cons of ConversationTokenBufferMemory:</b>\n",
    "\n",
    "    *Potential loss of context: Flushing interactions based on token length may result in the removal of earlier interactions that could contain important context or information, potentially affecting the accuracy of responses.\n",
    "    *Complexity in threshold setting: Determining the appropriate token count threshold for flushing interactions may require careful consideration and experimentation to find the optimal balance between memory usage and context retention.\n",
    "    *Difficulty in long-term context retention: Due to the dynamic nature of token-based flushing, retaining long-term context in the conversation may pose challenges as older interactions are more likely to be removed from the buffer.\n",
    "    *Impact on response quality: In situations where high-context conversations are required, the token-based flushing approach may lead to a reduction in the depth of understanding and the quality of responses.\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c919898",
   "metadata": {
    "id": "4c919898"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationTokenBufferMemory(llm=llm, max_token_limit=60),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2decc70c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2decc70c",
    "outputId": "2b9807d2-4d33-4ff1-dcf6-4b25aacdab13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c12417fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "c12417fd",
    "outputId": "9ca1eabb-3302-419b-9b58-719edc00d2a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Good morning AI!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Good morning! How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d66434e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "2d66434e",
    "outputId": "ea9dbbc0-8966-41bc-9ed3-b936125d1334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Good morning AI!\n",
      "AI: Good morning! How can I assist you today?\n",
      "Human: My Name is sharath\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Nice to meet you, Sharath! How can I assist you today?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"My Name is sharath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30364fb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "30364fb1",
    "outputId": "530bb596-2ecf-48d6-b593-088a11a8e462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Good morning AI!\n",
      "AI: Good morning! How can I assist you today?\n",
      "Human: My Name is sharath\n",
      "AI: Nice to meet you, Sharath! How can I assist you today?\n",
      "Human: I stay in hyderabad\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'That\\'s great! Hyderabad is a vibrant city known for its rich history and culture. It is the capital of the Indian state of Telangana and is also known as the \"City of Pearls\" and the \"City of Nizams.\" Hyderabad is famous for its iconic landmarks such as the Charminar, Golconda Fort, and Hussain Sagar Lake. It is also a major IT hub and home to many multinational companies. Is there anything specific you would like to know about Hyderabad?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I stay in hyderabad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37d5f7cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37d5f7cb",
    "outputId": "b1145daa-e03d-44ba-e59e-0fc748753839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a288b574",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "a288b574",
    "outputId": "d7b893ab-f6b9-4527-ab3d-e50c2d1a65f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bca5eb",
   "metadata": {
    "id": "d6bca5eb"
   },
   "source": [
    "### ***************END****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f7911",
   "metadata": {
    "id": "ff8f7911"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
